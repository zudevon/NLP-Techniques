{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939c2ed7",
   "metadata": {},
   "source": [
    "# Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6fcc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e95255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/devonrasch/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devonrasch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70f375d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    \"the quick brown fox\",\n",
    "    \"Jumps over $$$ the lazy brown dog\",\n",
    "    \"who jumps high into the blue sky after counting 123\",\n",
    "    \"And quickly returns to earth\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5623d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 2, 5],\n",
       " [3, 6, 1, 7, 2, 8],\n",
       " [9, 3, 10, 11, 1, 12, 13, 14, 15, 16],\n",
       " [17, 18, 19, 20, 21]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e54da",
   "metadata": {},
   "source": [
    " the word brown appears in lines one and two and is represented by the index two. Therefore two appears in both sequences. Similarly a three representing the word jumps appears in the sequences two and three. The index zero isn't used to denote words; it's reversed to serve as a padding.\n",
    "\n",
    " TLDR : \n",
    " * Index 0 for the 3 is reserved to serve as a padding. \n",
    " * `The` is found at $$$ representing 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fbce6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 4], [2, 5, 1, 6], [2, 7, 8, 9, 10], [11, 12, 13]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "lines = list(map(remove_stopwords, lines))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6128a",
   "metadata": {},
   "source": [
    "Neural networks expects all sequences be the same length Keira's pad sequences performs the final step truncating sequences longer than the specified length and padding sequence is shorter than the specified length with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b72162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  1,  4,  2,  5],\n",
       "       [ 0,  0,  0,  0,  3,  6,  1,  7,  2,  8],\n",
       "       [ 9,  3, 10, 11,  1, 12, 13, 14, 15, 16],\n",
       "       [ 0,  0,  0,  0,  0, 17, 18, 19, 20, 21]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef78c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEAT CODE\n",
    "# tokenizer = Tokenizer(filters='!\"@#$%^&*()-_=+:;[{]}\\|<>,./?\\t\\n[\\\\] `~ '\\\n",
    "#                       '0123456789')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
